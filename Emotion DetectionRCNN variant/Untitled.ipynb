{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Concatenate, Conv1D, Bidirectional, LSTM, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the string\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words(\"english\")  # List that contains stopwords to reduce noise\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    cleaned_text = text.lower()\n",
    "    cleaned_text=re.sub(\"[^a-zA-Z]\",\" \",cleaned_text) #extracting all the words\n",
    "    cleaned_text=re.sub(r'\\b\\w{1,3}\\b', '',cleaned_text) #removing words with less than 3\n",
    "\n",
    "    cleaned_text = \"\".join(c for c in cleaned_text if c not in punctuations) #removing punctuation from the data\n",
    "    words = cleaned_text.split()\n",
    "    words = [w for w in words if w not in stopword_list]\n",
    "    \n",
    "    \n",
    "     #lemmatization\n",
    "    words = [lem.lemmatize(word,\"v\") for word in words]\n",
    "    words = [lem.lemmatize(word,\"n\") for word in words]\n",
    "    words = [lem.lemmatize(word,\"r\") for word in words]\n",
    "    cleaned_text = \" \".join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    selected = ['label', 'tweet']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "    df = df.drop(non_selected, axis=1)\n",
    "    df = df.dropna(axis=0, how='any', subset=selected)\n",
    "    labels = sorted(list(set(df[selected[0]].tolist())))\n",
    "    dict.fromkeys(set(df[selected[0]].tolist()))\n",
    "    label_dict = {}\n",
    "    for i in range(len(labels)):\n",
    "        label_dict[labels[i]] = i\n",
    "\n",
    "    x_train = df[selected[1]].apply(lambda x: clean_str(x)).tolist()\n",
    "    y_train = df[selected[0]].apply(lambda y: label_dict[y]).tolist()\n",
    "    y_train = to_categorical(np.asarray(y_train))\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabAndData(sentences):\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    vocab = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=maxlen,padding='post')\n",
    "    return vocab,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 11\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "epochs = 5\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNNVariant(Model):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 maxlen,\n",
    "                 max_features,\n",
    "                 embedding_dims,\n",
    "                 kernel_sizes=[1, 2, 3, 4, 5],\n",
    "                 class_num=4,\n",
    "                 last_activation='sigmoid'):\n",
    "        super(RCNNVariant, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
    "        self.bi_rnn = Bidirectional(LSTM(128, return_sequences=True))\n",
    "        self.concatenate = Concatenate()\n",
    "        self.convs = []\n",
    "        for kernel_size in self.kernel_sizes:\n",
    "            conv = Conv1D(128, kernel_size, activation='relu')\n",
    "            self.convs.append(conv)\n",
    "        self.avg_pooling = GlobalAveragePooling1D()\n",
    "        self.max_pooling = GlobalMaxPooling1D()\n",
    "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs.get_shape()) != 2:\n",
    "            raise ValueError('The rank of inputs of TextRNN must be 2, but now is %d' % len(inputs.get_shape()))\n",
    "        if inputs.get_shape()[1] != self.maxlen:\n",
    "            raise ValueError('The maxlen of inputs of TextRNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
    "        embedding = self.embedding(inputs)\n",
    "        x_context = self.bi_rnn(embedding)\n",
    "        x = self.concatenate([embedding, x_context])\n",
    "        convs = []\n",
    "        for i in range(len(self.kernel_sizes)):\n",
    "            conv = self.convs[i](x)\n",
    "            convs.append(conv)\n",
    "        poolings = [self.avg_pooling(conv) for conv in convs] + [self.max_pooling(conv) for conv in convs]\n",
    "        x = self.concatenate(poolings)\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RCNNVariant(maxlen, max_features, embedding_dims)\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = loadData('tweetdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data = createVocabAndData(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=TEST_SPLIT, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 87s 329ms/step - loss: 0.2587 - accuracy: 0.7627 - val_loss: 0.1207 - val_accuracy: 0.9174\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 92s 348ms/step - loss: 0.0716 - accuracy: 0.9474 - val_loss: 0.1083 - val_accuracy: 0.9155\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 94s 354ms/step - loss: 0.0453 - accuracy: 0.9640 - val_loss: 0.1159 - val_accuracy: 0.9174\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 99s 372ms/step - loss: 0.0327 - accuracy: 0.9692 - val_loss: 0.1435 - val_accuracy: 0.9118\n",
      "Model: \"rcnn_variant_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  3000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection multiple                  439296    \n",
      "_________________________________________________________________\n",
      "concatenate_2 (Concatenate)  multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           multiple                  71296     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           multiple                  142464    \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           multiple                  213632    \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           multiple                  284800    \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           multiple                  355968    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  5124      \n",
      "=================================================================\n",
      "Total params: 4,512,580\n",
      "Trainable params: 4,512,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
