{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dir='Glove/'\n",
    "MAX_SEQUENCE_LENGTH = 11\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 300\n",
    "TEST_SPLIT = 0.1\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the string\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words(\"english\")  # List that contains stopwords to reduce noise\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    cleaned_text = text.lower()\n",
    "    cleaned_text=re.sub(\"[^a-zA-Z]\",\" \",cleaned_text) #extracting all the words\n",
    "    cleaned_text=re.sub(r'\\b\\w{1,3}\\b', '',cleaned_text) #removing words with less than 3\n",
    "\n",
    "    cleaned_text = \"\".join(c for c in cleaned_text if c not in punctuations) #removing punctuation from the data\n",
    "    words = cleaned_text.split()\n",
    "    words = [w for w in words if w not in stopword_list]\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "    words = words.str.replace(pattern, r\"\\1\")\n",
    "    \n",
    "     #lemmatization\n",
    "    words = [lem.lemmatize(word,\"v\") for word in words]\n",
    "    words = [lem.lemmatize(word,\"n\") for word in words]\n",
    "    words = [lem.lemmatize(word,\"r\") for word in words]\n",
    "    cleaned_text = \" \".join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloveVec(filename):\n",
    "    embeddings = {}\n",
    "    f = open(os.path.join(Dir, filename), encoding='utf-8')\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        except ValueError:\n",
    "            i += 1\n",
    "    f.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    selected = ['label', 'tweet']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "    df = df.drop(non_selected, axis=1)\n",
    "    df = df.dropna(axis=0, how='any', subset=selected)\n",
    "    labels = sorted(list(set(df[selected[0]].tolist())))\n",
    "    dict.fromkeys(set(df[selected[0]].tolist()))\n",
    "    label_dict = {}\n",
    "    for i in range(len(labels)):\n",
    "        label_dict[labels[i]] = i\n",
    "\n",
    "    x_train = df[selected[1]].apply(lambda x: clean_str(x)).tolist()\n",
    "    y_train = df[selected[0]].apply(lambda y: label_dict[y]).tolist()\n",
    "    y_train = to_categorical(np.asarray(y_train))\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabAndData(sentences):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    vocab = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "    return vocab,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEmbeddingMatrix(word_index,embeddings_index):\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "callbacks = [EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=5),\n",
    "             ModelCheckpoint(filepath='best_model.h5', #Best model gets saved\n",
    "             monitor='val_loss',mode='min',save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmModel(embedding_matrix,epoch):\n",
    "    model = Sequential()\n",
    "    n, embedding_dims = embedding_matrix.shape\n",
    "\n",
    "    model.add(Embedding(n, embedding_dims, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True,mask_zero=True))\n",
    "    model.add(LSTM(128,dropout=0.1,recurrent_dropout=0.2))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(4,activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, epochs=epoch, batch_size=128,callbacks=callbacks)\n",
    "    model.save_weights('text_lstm_weights.h5')\n",
    "\n",
    "    scores= model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created\n",
      "Train Test split\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 11, 300)           2340900   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,569,064\n",
      "Trainable params: 2,569,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "23/23 [==============================] - 16s 696ms/step - loss: 1.2479 - accuracy: 0.4550 - val_loss: 1.0565 - val_accuracy: 0.5890\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 8s 364ms/step - loss: 0.7959 - accuracy: 0.7152 - val_loss: 0.7749 - val_accuracy: 0.7025\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 8s 364ms/step - loss: 0.4404 - accuracy: 0.8468 - val_loss: 0.6327 - val_accuracy: 0.7669\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 8s 364ms/step - loss: 0.2384 - accuracy: 0.9244 - val_loss: 0.5969 - val_accuracy: 0.7945\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 8s 361ms/step - loss: 0.1295 - accuracy: 0.9610 - val_loss: 0.5929 - val_accuracy: 0.7883\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 8s 365ms/step - loss: 0.0894 - accuracy: 0.9757 - val_loss: 0.6094 - val_accuracy: 0.8006\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 8s 358ms/step - loss: 0.0607 - accuracy: 0.9826 - val_loss: 0.6886 - val_accuracy: 0.7822\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 8s 357ms/step - loss: 0.0504 - accuracy: 0.9839 - val_loss: 0.6558 - val_accuracy: 0.7975\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 8s 361ms/step - loss: 0.0462 - accuracy: 0.9846 - val_loss: 0.6820 - val_accuracy: 0.8037\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 9s 370ms/step - loss: 0.0388 - accuracy: 0.9891 - val_loss: 0.6990 - val_accuracy: 0.8006\n",
      "Epoch 00010: early stopping\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 0.6766 - accuracy: 0.8398\n",
      "accuracy: 83.98%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sentences, labels = loadData('tweets.csv')\n",
    "    embeddings = gloveVec('glove.6B.300d.txt')\n",
    "    vocab, data = createVocabAndData(sentences)\n",
    "    embedding_mat = createEmbeddingMatrix(vocab,embeddings)\n",
    "    pickle.dump([data, labels, embedding_mat], open('embedding_matrix.pkl', 'wb'))\n",
    "    print (\"Data created\")\n",
    "\n",
    "    print(\"Train Test split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=TEST_SPLIT, random_state=42)\n",
    "\n",
    "    lstmModel(embedding_mat,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_gru = [EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=5),\n",
    "             ModelCheckpoint(filepath='best_model_gru.h5', \n",
    "             monitor='val_loss',mode='min',save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRUModel(embedding_matrix,epoch):\n",
    "    model = Sequential()\n",
    "    n, embedding_dims = embedding_matrix.shape\n",
    "\n",
    "    model.add(Embedding(n, embedding_dims, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True,mask_zero=True))\n",
    "    model.add(GRU(128,dropout=0.1,recurrent_dropout=0.2))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(4,activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, epochs=epoch, batch_size=128,callbacks=callbacks_gru)\n",
    "    model.save_weights('text_gru_weights.h5')\n",
    "\n",
    "    scores= model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created\n",
      "Train Test split\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 11, 300)           2340900   \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 128)               165120    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,514,536\n",
      "Trainable params: 2,514,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "23/23 [==============================] - 34s 1s/step - loss: 1.2278 - accuracy: 0.4687 - val_loss: 1.0549 - val_accuracy: 0.5951\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 9s 393ms/step - loss: 0.8118 - accuracy: 0.6995 - val_loss: 0.7386 - val_accuracy: 0.7055\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 7s 311ms/step - loss: 0.4504 - accuracy: 0.8434 - val_loss: 0.6091 - val_accuracy: 0.7730\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 7s 304ms/step - loss: 0.2419 - accuracy: 0.9159 - val_loss: 0.6267 - val_accuracy: 0.7638\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 7s 307ms/step - loss: 0.1334 - accuracy: 0.9607 - val_loss: 0.6044 - val_accuracy: 0.7822\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 7s 304ms/step - loss: 0.0926 - accuracy: 0.9730 - val_loss: 0.5754 - val_accuracy: 0.8098\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0698 - accuracy: 0.9771 - val_loss: 0.5745 - val_accuracy: 0.8190\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 7s 311ms/step - loss: 0.0528 - accuracy: 0.9826 - val_loss: 0.5969 - val_accuracy: 0.8252\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 7s 307ms/step - loss: 0.0475 - accuracy: 0.9832 - val_loss: 0.6112 - val_accuracy: 0.8098\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0424 - accuracy: 0.9846 - val_loss: 0.6432 - val_accuracy: 0.8221\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 7s 309ms/step - loss: 0.0357 - accuracy: 0.9853 - val_loss: 0.6721 - val_accuracy: 0.8098\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 7s 305ms/step - loss: 0.0420 - accuracy: 0.9846 - val_loss: 0.7398 - val_accuracy: 0.7975\n",
      "Epoch 00012: early stopping\n",
      "accuracy: 83.43%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sentences, labels = loadData('tweets.csv')\n",
    "    embeddings = gloveVec('glove.6B.300d.txt')\n",
    "    vocab, data = createVocabAndData(sentences)\n",
    "    embedding_mat = createEmbeddingMatrix(vocab,embeddings)\n",
    "    pickle.dump([data, labels, embedding_mat], open('embedding_matrix.pkl', 'wb'))\n",
    "    print (\"Data created\")\n",
    "\n",
    "    print(\"Train Test split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=TEST_SPLIT, random_state=42)\n",
    "\n",
    "    GRUModel(embedding_mat,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
